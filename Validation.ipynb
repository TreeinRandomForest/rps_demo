{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffaa47db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as tfms\n",
    "from typing import List\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "from data import get_ds_dl\n",
    "from model import get_resnet\n",
    "from export import create_session, predict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175bc913",
   "metadata": {},
   "source": [
    "### Test ONNX on files from validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ca2cc4",
   "metadata": {},
   "source": [
    "##### Change the following cell\n",
    "\n",
    "onnx_file: point to which model you want to test\n",
    "\n",
    "fname_list: pick which set of images you want to run the model over to evaluate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16c7ebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHANGE THIS TO TEST MODEL\n",
    "train_folder = '../Rock-Paper-Scissors/train_aug2425_edges/'\n",
    "test_folder = '../Rock-Paper-Scissors/test_edges/'\n",
    "\n",
    "onnx_file = 'test_edges.onnx'\n",
    "\n",
    "fname_list = test_fnames #PICK WHICH IMAGES YOU WANT TO TEST OVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f22271b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_test, dl_train, dl_test = get_ds_dl(None, \n",
    "                                                 1,\n",
    "                                                 train_folder,\n",
    "                                                 test_folder)\n",
    "classes = ds_train.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "499e6891",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fnames = glob.glob(f'{train_folder}/*/*')\n",
    "test_fnames = glob.glob(f'{test_folder}/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2695d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, transform = get_resnet()\n",
    "transform = tfms.Compose([transform, \n",
    "                          tfms.RandomRotation(45),\n",
    "                          tfms.RandomHorizontalFlip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "beeafdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = create_session(onnx_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "945e3852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "372\n"
     ]
    }
   ],
   "source": [
    "print(len(train_fnames))\n",
    "print(len(test_fnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "846ced61",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#fname_list = train_fnames\n",
    "#fname_list = test_fnames #PICK WHICH IMAGES YOU WANT TO TEST OVER\n",
    "plot = False\n",
    "total = 0\n",
    "incorrect = []\n",
    "\n",
    "for idx in range(len(fname_list)):\n",
    "    fname = fname_list[idx]\n",
    "\n",
    "    #get predictions and labels\n",
    "    pred = predict(sess, fname, transform)\n",
    "    label = fname.split(\"/\")[-2]\n",
    "    pred_class = classes[pred.argmax()]\n",
    "\n",
    "    if label==pred_class: #only look at incorrect predictions\n",
    "        continue\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        plt.imshow(Image.open(fname).convert('RGB')) #plot raw image\n",
    "        _ = plt.title(f'label = {label} pred = {pred_class} \\n pred_logits = {pred}\\n classes = {classes}')\n",
    "    \n",
    "    total += 1 #a counter to limit how many examples being looked at\n",
    "    incorrect.append((label, pred_class, pred))\n",
    "    #if total > 10:\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ab1695",
   "metadata": {},
   "source": [
    "##### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34ddcc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of incorrect preds: 0.185\n"
     ]
    }
   ],
   "source": [
    "print(f\"% of incorrect preds: {len(incorrect) / len(fname_list):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88344ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of incorrect predictions:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th>pred_class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>paper</th>\n",
       "      <th>rock</th>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">scissors</th>\n",
       "      <th>paper</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rock</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     count\n",
       "label    pred_class       \n",
       "paper    rock           24\n",
       "scissors paper          15\n",
       "         rock           30"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Distribution of incorrect predictions:\")\n",
    "df = pd.DataFrame({'label': [i[0] for i in incorrect], 'pred_class': [i[1] for i in incorrect]})\n",
    "df['count'] = 1\n",
    "df.groupby(['label', 'pred_class']).count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
